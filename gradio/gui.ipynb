{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96806490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3bddb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 64, 64, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 32, 32, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 4, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 4, 4, 32)          18464     \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 2, 2, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,492\n",
      "Trainable params: 79,492\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "# 使用 Sequential 定義模型\n",
    "model = models.Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(layers.Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(64, 64, 3)))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "model.add(layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 4\n",
    "model.add(layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 5\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten Layer\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully Connected Layer\n",
    "model.add(layers.Dense(4))\n",
    "\n",
    "# 檢查模型結構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91668a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = '../tf_model_no_batchnorm_v3/TFModel.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c519081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If successfully loaded the model, it will return None\n",
    "# model = model.load_weights(weight_path, by_name=True)\n",
    "model.load_weights(weight_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65119db1",
   "metadata": {},
   "source": [
    "Note: here is a simple test, so the audio file will draw mel-spectrogram with python version\n",
    "So, the model predictions may be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe46b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Image Shape: (1, 64, 64, 3)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Predicted result:  boat+whale\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# 定義圖片大小\n",
    "image_size = (64, 64)\n",
    "# 字典對應表\n",
    "dict_for_label = {\n",
    "    'boat': [1, 0, 0, 0], 'dolphin': [0, 1, 0, 0], 'fish': [0, 0, 1, 0], 'whale': [0, 0, 0, 1],\n",
    "    'boat+fish': [1, 0, 1, 0], 'boat+whale': [1, 0, 0, 1],\n",
    "    'dolphin+boat': [1, 1, 0, 0], 'dolphin+whale': [0, 1, 0, 1], 'dolphin+fish': [0, 1, 1, 0],\n",
    "    'fish+whale': [0, 0, 1, 1]\n",
    "}\n",
    "# 將字典反過來查詢值\n",
    "# list 不能當 key，所以轉成 tuple 的 string\n",
    "pred_for_label = {str(tuple(v)): k for k, v in dict_for_label.items()}\n",
    "\n",
    "def preprocess_image(image_path, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    將圖片處理為符合模型輸入的格式\n",
    "    :param image_path: 圖片路徑\n",
    "    :param target_size: 模型輸入的目標大小\n",
    "    :return: 處理後的圖片數據，形狀為 (1, height, width, channels)\n",
    "    \"\"\"\n",
    "    # 加載圖片並調整大小\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img_array = img_to_array(img) / 255.0  # 標準化到 [0, 1]\n",
    "    return np.expand_dims(img_array, axis=0)  # 增加 batch 維度\n",
    "\n",
    "def predict_image_class(image_path, model):\n",
    "    \"\"\"\n",
    "    預測圖片類別\n",
    "    :param image_path: 圖片路徑\n",
    "    :param model: 訓練好的模型\n",
    "    :return: 預測結果\n",
    "    \"\"\"\n",
    "\n",
    "    # Add batch dimension to the image\n",
    "    processed_image = preprocess_image(image_path)\n",
    "    print(\"Processed Image Shape:\", processed_image.shape)\n",
    "    predictions = model.predict(processed_image)\n",
    "    # 計算每個類別的預測概率（Sigmoid 函數處理）\n",
    "    predicted_prob = tf.sigmoid(predictions)\n",
    "    # 將每個類別的預測概率與閾值 0.5 比較，大於 0.5 表示該類別存在 (標記為 1)\n",
    "    predicted = tf.cast(predicted_prob > 0.5, tf.int32)\n",
    "    return np.array(predicted).squeeze()  # 去除多餘的維度\n",
    "\n",
    "\n",
    "# 測試圖片路徑\n",
    "test_image_path = r'20201016_070712.png'  # 替換為你的圖片路徑\n",
    "\n",
    "# 檢查處理後的圖片形狀\n",
    "predictions = predict_image_class(test_image_path, model)\n",
    "print(\"Predicted result: \", pred_for_label[str(tuple(predictions))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "21cef2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio params\n",
    "SAMPLE_RATE = 22050  # (samples/sec)\n",
    "DURATION = 5.0  # duration in second (sec)\n",
    "AUDIO_LEN = int(SAMPLE_RATE * DURATION)  # total number of samples in DURATION\n",
    "\n",
    "# Spectrogram params\n",
    "N_MELS = 128  # freq axis, number of filters\n",
    "N_FFT = 2048  # frame size\n",
    "HOP_LEN = 512  # non-overlap region, which means 1/4 portion overlapping\n",
    "SPEC_WIDTH = AUDIO_LEN // HOP_LEN + 1  # time axis\n",
    "FMAX = SAMPLE_RATE // 2  # max frequency, based on the rule, it should be half of SAMPLE_RATE\n",
    "SPEC_SHAPE = [N_MELS, SPEC_WIDTH]  # expected output spectrogram shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e4190fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "def prediction(audio_input):\n",
    "    \"\"\"\n",
    "    Process audio input, generate a spectrogram, and predict the sound type.\n",
    "    \n",
    "    Args:\n",
    "        audio_input: Audio input from Gradio (tuple of (sample_rate, audio_data))\n",
    "        \n",
    "    Returns:\n",
    "        String prediction result\n",
    "    \"\"\"\n",
    "    # Check if audio input is provided\n",
    "    if audio_input is None:\n",
    "        return \"請提供音頻檔案\"\n",
    "    \n",
    "    # Gradio audio input format is (sample_rate, audio_data)\n",
    "    sr, audio_data = audio_input\n",
    "    # transfer to float32 (Because librosa need float32)\n",
    "    audio_data = audio_data.astype(np.float32)\n",
    "\n",
    "    # Ensure audio_data is numpy array\n",
    "    if not isinstance(audio_data, np.ndarray):\n",
    "        return \"音頻數據格式錯誤\"\n",
    "    \n",
    "    # Create a temporary file name for the output spectrogram\n",
    "    temp_filename = \"temp_audio.png\"\n",
    "    \n",
    "    # Generate the mel spectrogram directly from the audio data\n",
    "    spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data, sr=sr, fmax=FMAX, \n",
    "        n_mels=N_MELS, hop_length=HOP_LEN, n_fft=N_FFT\n",
    "    )\n",
    "    spec = librosa.power_to_db(spec)\n",
    "    \n",
    "    # Plot the mel spectrogram\n",
    "    plt.figure()\n",
    "    librosa.display.specshow(\n",
    "        spec, sr=sr, hop_length=HOP_LEN, \n",
    "        x_axis='time', y_axis='mel', cmap='viridis'\n",
    "    )\n",
    "    plt.title(\"Spectrogram_temp_audio\", fontsize=17)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the spectrogram image\n",
    "    plt.savefig(temp_filename)\n",
    "    plt.close()\n",
    "    \n",
    "    # Print the path for debugging\n",
    "    print(f\"Saved spectrogram to {temp_filename}\")\n",
    "    \n",
    "    # Use the model to predict the class\n",
    "    predictions = predict_image_class(temp_filename, model)\n",
    "    result = pred_for_label[str(tuple(predictions))] # a string of class result\n",
    "    \n",
    "    # Load and display the appropriate images based on prediction\n",
    "    class_result = result.split('+')\n",
    "    images = []\n",
    "    \n",
    "    # Create an image for each detected class\n",
    "    for res in class_result:\n",
    "        if res == 'boat':\n",
    "            img_path = './images/boat.jpg'\n",
    "        elif res == 'dolphin':\n",
    "            img_path = './images/dolphin.jpg'\n",
    "        elif res == 'fish':\n",
    "            img_path = './images/fish.jpg'\n",
    "        elif res == 'whale':\n",
    "            img_path = './images/whale.jpg'\n",
    "        \n",
    "        # Check if image file exists\n",
    "        if os.path.exists(img_path):\n",
    "            img = plt.imread(img_path)\n",
    "            images.append(img)\n",
    "        else:\n",
    "            print(f\"Warning: Image file {img_path} not found\")\n",
    "    \n",
    "    # Combine images into a single figure if there are multiple classes\n",
    "    if len(images) > 0:\n",
    "        fig, axs = plt.subplots(1, len(images), figsize=(5*len(images), 5))\n",
    "        \n",
    "        # Handle the case of a single image\n",
    "        if len(images) == 1:\n",
    "            axs.imshow(images[0])\n",
    "            axs.axis('off')\n",
    "            axs.set_title(class_result[0])\n",
    "        else:\n",
    "            for i, (img, cls) in enumerate(zip(images, class_result)):\n",
    "                axs[i].imshow(img)\n",
    "                axs[i].axis('off')\n",
    "                axs[i].set_title(cls)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        result_img_path = \"prediction_result.png\"\n",
    "        plt.savefig(result_img_path)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Return both the text prediction and the image\n",
    "        return f\"預測結果: {result}\", PIL.Image.open(result_img_path)\n",
    "    \n",
    "    # If no images were found, return only the text prediction\n",
    "    return f\"預測結果: {result}\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7b30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved spectrogram to temp_audio.png\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "# create a Gradio interface\n",
    "demo = gr.Interface(\n",
    "        fn = prediction,\n",
    "        inputs = [gr.Audio()],\n",
    "        outputs = [\"text\", \"image\"],       # get prediction result\n",
    "        title = '水下聲音辨識測試',\n",
    "        description = '可預測聲音類別為：船、海豚、魚、鯨魚'\n",
    ")\n",
    "\n",
    "# Show demo\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea43e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
